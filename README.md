# Deep Learning Mastery ‚Äî 4 Week Intensive Program

This repository contains a **rigorous and structured 4-week plan** designed to rebuild, deepen, and master:

- Machine Learning & Deep Learning  
- Mathematics for ML  
- Advanced Optimization  
- Self-Supervised Learning (SimCLR, MoCo, BYOL, SimSiam, DINO, iBOT)  
- Few-Shot Learning (ProtoNet, MatchingNets, RelationNets, CLIP)  
- Transformers & Vision Transformers (ViT, DeiT, Swin)  
- Explainable AI (Grad-CAM, IG, SHAP, LIME)  
- Diffusion Models  
- ML System Design  

Each week includes:  
‚úî **Exact topics**  
‚úî **Precise deliverables**  
‚úî **Actionable, chapter-specific and section-specific resources**  
‚úî **Paper-specific instructions**  

---

# üìÖ 4-Week Schedule Overview

---

# **WEEK 1 ‚Äî Mathematics for Machine Learning**  
*Goal: Rebuild mathematical foundations required for DL, SSL, transformers & advanced optimization.*

---

## **TOPICS**

### **1. Linear Algebra**
- Matrix calculus (Jacobian, Hessian)
- Eigendecomposition, SVD
- Vector norms & induced matrix norms
- Quadratic forms  
- Vector-Jacobian product (VJP), JVP  

**Resources (Specific Chapters):**
- **Matrix Cookbook** ‚Üí Sections: *Derivatives*, *Identities*  
- **Deep Learning Book (Goodfellow)** ‚Üí Chapter **2**  
- **CS229 Linear Algebra Review** ‚Üí Entire PDF  

**Action:** Work through **all derivative identities** & implement them in Week-1 notebooks.

---

### **2. Probability & Statistics**
- PDFs, PMFs, CDFs  
- Expectation, variance, covariance matrix  
- MLE, MAP  
- KL Divergence derivation  
- CE‚ÄìNLL connection  

**Resources:**
- **All of Statistics (Wasserman)** ‚Üí Chapters **1, 2, 3, 7, 8, 11**  
- **CS229 Probability Review**  

**Action:** Solve 10‚Äì15 exercises from each chapter (summary only in notebook).

---

### **3. Optimization Theory**
- Convexity & smoothness  
- Gradient descent convergence  
- Lagrange multipliers  
- Stochastic gradients (variance analysis)  

**Resources:**
- **Convex Optimization (Boyd)** ‚Üí Chapters **1‚Äì4**, **9.1‚Äì9.3**  
- MIT 6.036 Optimization Lectures  

---

### **4. Information Theory**
- Entropy  
- Cross entropy  
- Mutual Information  
- InfoNCE derivation  

**Resources:**
- *Elements of Information Theory (Cover & Thomas)* ‚Üí Chapters **1‚Äì2**  
- CPC paper (Section: *Contrastive Predictive Coding Loss*)  

---

## **DELIVERABLES (VERY CLEAR)**

### **1. `matrix_calculus.ipynb`**
Implement by hand:
- Gradient of softmax  
- Gradient of CE loss  
- Gradient of quadratic form x·µÄAx  
- Derivative of sigmoid, tanh, GELU  

---

### **2. `probability_kl.ipynb`**
Include:
- KL divergence between two Gaussians (derive)  
- CE = NLL proof (write steps)  
- MLE of mean & variance for Normal distribution  
- Bayes rule toy examples  

---

### **3. `optimization_algorithms.ipynb`**
Implement from scratch:
- GD, SGD, Momentum  
- RMSProp  
- Adam  
- Plot convergence curves on a quadratic bowl & a simple NN  

---

### **4. `math_summary.pdf`**  
One-page notes (your own summary).

---

# **WEEK 2 ‚Äî Core Deep Learning + Loss Functions + Modern Training**

---

## **TOPICS**

### **1. Neural Network Foundations**
- Backprop through computation graph  
- Xavier & Kaiming initialization  
- Normalization techniques: BN, LN, GN, RMSNorm  

**Resources:**
- **Deep Learning Book** ‚Üí Chapters **6, 7, 8**  
- **CS231n** ‚Üí Backprop Notes  

---

### **2. Loss Functions (Deep Dive)**
You must **derive the formula**, **implement it**, and **train a model** using each.

Losses to cover:
- MSE, MAE  
- Cross-entropy  
- KL-divergence  
- Label smoothing  
- Focal Loss ‚Üí *RetinaNet Paper Section 3*  
- Triplet Loss ‚Üí *FaceNet Paper Section 4.2*  
- Contrastive Loss ‚Üí *SimCLR Paper Section 3.4*  
- InfoNCE ‚Üí *CPC Paper Section 2.2*  
- NT-Xent ‚Üí *SimCLR Paper Equation (1)*  
- Dice Loss  
- IoU Loss  

---

### **3. Modern Training Techniques**
- SGD-M, AdamW  
- Cosine LR, Warmup  
- AMP (Mixed Precision)  
- Gradient Clipping  
- SAM (Section 3 of the SAM Paper)  

---

### **4. CNN Architectures**
Implement & compare:
- ResNet (ResNet paper Sections 3.3‚Äì4)  
- EfficientNet (compound scaling idea only)  
- ConvNeXt (Sections 3‚Äì4)  

---

## **DELIVERABLES**

### **1. `resnet_from_scratch/`**
- Build ResNet-18 from scratch (PyTorch)  
- Train on CIFAR-10  
- **Required: 85%+ test accuracy**  
- Include training curves  

---

### **2. `loss_functions/`**
Notebooks:
- `basic_losses.ipynb` ‚Üí MSE, CE, KL  
- `ranking_losses.ipynb` ‚Üí Triplet, Contrastive  
- `segmentation_losses.ipynb` ‚Üí Dice, IoU  
- `ssl_losses.ipynb` ‚Üí InfoNCE, NT-Xent  

Each notebook must include:
- Formula  
- Gradient sketch  
- PyTorch implementation  
- Comparison plots  

---

### **3. `modern_training/optimizer_benchmarks.ipynb`**
- Benchmark Adam, AdamW, SGD-M, RMSProp  
- Compare convergence + generalization  

---

### **4. `training_tricks_summary.pdf`**
Your own summary.

---

# **WEEK 3 ‚Äî SSL, Contrastive Learning, DINO, FSL & Transformers**

---

## **TOPICS**

### **1. Contrastive Learning**
Study **AND reproduce**:
- SimCLR ‚Üí Sections **3.1‚Äì3.4**  
- MoCo v2 ‚Üí Sections **3‚Äì4**  
- Temperature scaling analysis  
- Momentum encoder update (MoCo Eq. (2))  

---

### **2. Self-Supervised Learning (SSL) Pipelines**
**Read + extract key mechanisms:**

| Method | Required Sections to Study |
|--------|---------------------------|
| **SimCLR** | Entire Section 3 (architecture, loss, augmentations) |
| **MoCo v2** | Sections 2‚Äì4 |
| **BYOL** | Sections 1‚Äì4 (stop-grad mechanism) |
| **SimSiam** | Sections 3‚Äì5 (no negatives) |
| **DINO** | Sections 3‚Äì4 (student-teacher, centering, temperature) |
| **iBOT** | Sections 2‚Äì4 (MIM + contrastive) |

---

### **3. Few-Shot Learning**
Study and implement:
- Prototypical Networks ‚Üí Sections 2‚Äì3  
- Matching Networks ‚Üí Sections 2‚Äì4  
- Relation Networks ‚Üí Sec 3  
- CLIP FSL ‚Üí Zero-shot retrieval (OpenAI CLIP paper Sections 2‚Äì4)  

---

### **4. Transformers**
Study and implement:
- Attention derivation ‚Üí *Attention Is All You Need* Section **3.2**  
- MHA ‚Üí Section **3.2.2**  
- Positional Encodings ‚Üí Section **3.5**  
- ViT ‚Üí Sections **3‚Äì4**  
- DeiT ‚Üí Sections **3‚Äì5**  
- Swin ‚Üí Sections **3‚Äì4**  

---

## **DELIVERABLES**

### **SIMCLR**
Folder: `simclr/`
- `nt_xent.py` ‚Äî your implementation  
- `train_simclr.py` ‚Äî full training  
- `augmentations.py` ‚Äî SimCLR augmentations  
- Train on CIFAR-10  
- **Linear probe accuracy ‚â• 75%**  

---

### **DINO**
Folder: `dino/`
- Implement student‚Äìteacher  
- Implement centering  
- Implement temperature sharpening  
- Train on CIFAR-10 for 100 epochs  
- Extract features + run KMeans (k=10)  

---

### **Few-Shot**
- `protonet.ipynb`  
- `matching_nets.ipynb`  
- `relation_nets.ipynb`  
- Evaluate 5-way 1-shot accuracy  

---

### **Transformers**
- `attention_from_scratch.ipynb` ‚Üí derive attention step by step  
- `tiny_transformer.py` ‚Üí build a mini Transformer  
- `vit_scratch.py` ‚Üí patchify + encoder  

---

### **SSL Comparison**
- `ssl_comparison.pdf` ‚Üí 2 pages summarizing all methods  

---

# **WEEK 4 ‚Äî XAI, Advanced CV, Diffusion, ML System Design**

---

## **TOPICS**

### **Explainable AI**
Study:  
- Grad-CAM  
- Grad-CAM++  
- Integrated Gradients (IG paper Section 3)  
- SHAP  
- LIME  
- Attention rollout for ViTs  

---

### **Advanced CV Architectures**
Study specific sections:
- U-Net ‚Üí Sections **2‚Äì3**  
- DeepLab v3 ‚Üí Section **3**  
- Mask R-CNN ‚Üí Section **3**  
- DETR ‚Üí Sections **4‚Äì5**  
- YOLOv8 ‚Üí architecture overview  

---

### **Diffusion Models**
Study & reproduce:  
- DDPM ‚Üí Sections **2‚Äì3**  
- Reparameterization of noise schedule  
- Reverse denoising process  
- Classifier-free guidance  

---

### **ML System Design**
Study and create:
- Data pipelines  
- Feature stores  
- Model monitoring (prediction drift, data drift)  
- Deployment patterns  
- Vector DBs for embeddings  

---

## **DELIVERABLES**

### **1. Explainable AI Implementation**
Folder: `explainable_ai/`
- `grad_cam.py` + heatmaps  
- `integrated_gradients.py`  
- SHAP demo on ResNet  
- ViT attention rollout  

---

### **2. Advanced Vision Model**
Implement one:
- U-Net  
- DeepLab v3  
- DETR  

Include:
- Training script  
- Evaluation metrics  

---

### **3. Diffusion**
- `ddpm_scratch.ipynb`  
Includes:  
- Forward diffusion  
- Reverse sampling  
- Generate sample images  

---

### **4. ML System Design**
- `ml_system_design_doc.md`  
- Contains: pipelines, feature store, deployment graph  

---

### **5. CLIP Retrieval Demo**
Folder: `clip_retrieval_demo/`
- `encode_image.py`  
- `encode_text.py`  
- `retrieval_demo.ipynb`  

---

# üìö RESOURCE LIST (With Chapter/Section Specific Guidance)

---

## **Mathematics**
- *Matrix Cookbook* ‚Üí Derivatives + identities  
- *All of Statistics* ‚Üí Ch 1‚Äì3, 7‚Äì8, 11  
- *Convex Optimization (Boyd)* ‚Üí Ch 1‚Äì4, 9.1‚Äì9.3  
- CS229 Math Review ‚Üí Entire  
- *Elements of Information Theory* ‚Üí Ch 1‚Äì2  

---

## **Deep Learning**
- *Deep Learning Book* ‚Üí Ch 6‚Äì8  
- CS231n ‚Üí Backprop + CNNs  
- FastAI ‚Üí Modern training methods  

---

## **Loss Functions-Related Papers**
- **Focal Loss (RetinaNet)** ‚Üí Section 3  
- **Triplet Loss (FaceNet)** ‚Üí Section 4.2  
- **Dice Loss** ‚Üí 2016 medical imaging paper (Section 2)  
- **NT-Xent** ‚Üí SimCLR Eq. (1)  
- **InfoNCE** ‚Üí CPC Section 2.2  

---

## **Self-Supervised Learning Papers**
- **SimCLR** ‚Üí Sections 3‚Äì4  
- **MoCo v2** ‚Üí Sections 2‚Äì4  
- **BYOL** ‚Üí Sections 1‚Äì4  
- **SimSiam** ‚Üí Sections 3‚Äì5  
- **DINO** ‚Üí Sections 3‚Äì4  
- **iBOT** ‚Üí Sections 2‚Äì4  

---

## **Few-Shot Learning Papers**
- **Prototypical Networks** ‚Üí Sections 2‚Äì3  
- **Matching Networks** ‚Üí Sections 2‚Äì4  
- **Relation Networks** ‚Üí Section 3  
- **CLIP** ‚Üí Sections 2‚Äì4  

---

## **Transformers Papers**
- **Attention Is All You Need** ‚Üí Sections 3.2, 3.5  
- **ViT** ‚Üí Sections 3‚Äì4  
- **DeiT** ‚Üí Sections 3‚Äì5  
- **Swin** ‚Üí Sections 3‚Äì4  
- **Annotated Transformer** ‚Üí Entire walkthrough  

---

## **Explainable AI**
- **Grad-CAM** ‚Üí Entire paper  
- **Grad-CAM++** ‚Üí Sec 3  
- **Integrated Gradients** ‚Üí Sec 3  
- **SHAP Docs**  
- **LIME Docs**  

---

## **Diffusion**
- **DDPM** ‚Üí Sections 2‚Äì3  
- HF Diffusers ‚Üí Tutorials  

---

## **ML System Design**
- **Chip Huyen** ‚Üí Ch 2‚Äì7  
- **Google Rules of ML** ‚Üí Entire  
- **W&B Articles**  
- **FAISS Docs**  

---

# ‚≠ê END GOAL

By the end of this program you will have:

- Rebuilt complete ML mathematics  
- Implemented **ResNet, SimCLR, DINO, ProtoNet, Transformers, DDPM**  
- Learned XAI and ML System Design  
- Completed a polished GitHub portfolio suitable for ML Engineer roles  

---
